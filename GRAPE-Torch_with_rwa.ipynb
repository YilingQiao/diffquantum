{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "271d3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.special import legendre\n",
    "from scipy.stats import unitary_group\n",
    "from numpy import kron\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6597698",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grape(object):\n",
    "    \"\"\"A class for pulse-based simulation and optimization\n",
    "    Args:\n",
    "        taylor_terms: number of taylor expansion terms.\n",
    "    \"\"\"\n",
    "    def __init__(self, taylor_terms=5, n_step=100, n_epoch=200, lr=2e-2):\n",
    "        args = locals()\n",
    "        self.taylor_terms = taylor_terms\n",
    "        self.n_step = n_step\n",
    "        self.log_dir = \"./logs/\"\n",
    "        self.log_name = 'grape'\n",
    "        self.n_epoch = n_epoch\n",
    "        self.lr = lr\n",
    "\n",
    "        self.logger = Logger()\n",
    "        self.logger.write_text(\"arguments ========\")\n",
    "        for k, v in args.items():\n",
    "            if k == 'self':\n",
    "                continue\n",
    "            self.logger.write_text(\"{}: {}\".format(k, v))\n",
    "\n",
    "    def train_fidelity_ibm1(self, init_u, H0, Hs, initial_states, target_states):\n",
    "        \"\"\"Control the systems to reach target states.\n",
    "        Args:\n",
    "            init_u: initial pulse.\n",
    "            H0: a Hermitian matrix.\n",
    "            Hs: a list of Hermitian matrics.\n",
    "            initial_states: initial_states.\n",
    "            target_states: target_states.\n",
    "            dt: size of time step.\n",
    "        Returns:\n",
    "            us: optimized pulses.\n",
    "        \"\"\"\n",
    "        self.logger.write_text(\"!!!! train_fidelity ========\")\n",
    "\n",
    "        lr = self.lr\n",
    "        w_l2 = 0\n",
    "        \n",
    "        H0 = torch.tensor(self.c_to_r_mat(-1.j * self.dt * H0))\n",
    "        Hs = [torch.tensor(self.c_to_r_mat(-1.j * self.dt * hs)) for hs in Hs]\n",
    "        us = torch.tensor(init_u, requires_grad=True)\n",
    "        optimizer = torch.optim.Adam([us], lr=lr)\n",
    "        initial_states = torch.tensor(np.array(initial_states)).double().transpose(1, 0)\n",
    "        target_states = torch.tensor(np.array(target_states)).double().transpose(1, 0)\n",
    "        #sgm = torch.nn.Sigmoid()\n",
    "        self.losses_energy = []\n",
    "        for epoch in range(self.n_epoch):\n",
    "            st = \"params: {}\".format(us)\n",
    "            self.logger.write_text_aux(st)\n",
    "            final_states = self.forward_simulate_ibm1(us, H0, Hs, initial_states)\n",
    "            loss_fidelity = 1 - self.get_inner_product_2D(final_states, target_states)\n",
    "            loss_l2 = torch.sqrt((us**2).mean())\n",
    "            loss = loss_fidelity + loss_l2 * w_l2\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            st = \"epoch: {:04d}, loss: {:.16f}, loss_fid: {:.16f}\".format(\n",
    "                epoch, \n",
    "                float(loss.detach().numpy()), \n",
    "                float(loss_fidelity.detach().numpy()))\n",
    "            self.logger.write_text(st)\n",
    "\n",
    "            self.losses_energy.append(float(loss_fidelity.detach().numpy()))\n",
    "        \n",
    "        return us\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train_fidelity_ibm2(self, init_u, H0, Hs, initial_states, target_states):\n",
    "        \"\"\"Control the systems to reach target states.\n",
    "        Args:\n",
    "            init_u: initial pulse.\n",
    "            H0: a list of Hermitian matrices.\n",
    "            Hs: a list of Hermitian matrices.\n",
    "            initial_states: initial_states.\n",
    "            target_states: target_states.\n",
    "            dt: size of time step.\n",
    "        Returns:\n",
    "            us: optimized pulses.\n",
    "        \"\"\"\n",
    "        self.logger.write_text(\"!!!! train_fidelity ========\")\n",
    "\n",
    "        lr = self.lr\n",
    "        w_l2 = 0\n",
    "        #max_amplitude = torch.from_numpy(np.ones(len(Hs)))\n",
    "        H0 = [torch.tensor(self.c_to_r_mat(-1.j * self.dt * h0)) for h0 in H0]\n",
    "        Hs = [torch.tensor(self.c_to_r_mat(-1.j * self.dt * hs)) for hs in Hs]\n",
    "        us = torch.tensor(init_u, requires_grad=True)\n",
    "        optimizer = torch.optim.Adam([us], lr=lr)\n",
    "        initial_states = torch.tensor(np.array(initial_states)).double().transpose(1, 0)\n",
    "        target_states = torch.tensor(np.array(target_states)).double().transpose(1, 0)\n",
    "\n",
    "        self.losses_energy = []\n",
    "        for epoch in range(self.n_epoch):\n",
    "            st = \"params: {}\".format(us)\n",
    "            self.logger.write_text_aux(st)\n",
    "            \n",
    "            final_states = self.forward_simulate_ibm2(us, H0, Hs, initial_states)\n",
    "            loss_fidelity = 1 - self.get_inner_product_2D(final_states, target_states)\n",
    "            loss_l2 = torch.sqrt((us**2).mean())\n",
    "            loss = loss_fidelity + loss_l2 * w_l2\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            st = \"epoch: {:04d}, loss: {:.16f}, loss_fid: {:.16f}\".format(\n",
    "                epoch, \n",
    "                float(loss.detach().numpy()), \n",
    "                float(loss_fidelity.detach().numpy()))\n",
    "            self.logger.write_text(st)\n",
    "\n",
    "            self.losses_energy.append(float(loss_fidelity.detach().numpy()))\n",
    "        \n",
    "        return us\n",
    "\n",
    "    \n",
    "    def forward_simulate_ibm1(self, us, H0, Hs, initial_states):\n",
    "        \"\"\"Forward time evolution.\n",
    "        Args:\n",
    "            us: pulses.\n",
    "            H0: a Hermitian matrix.\n",
    "            Hs: a list of Hermitian matrics.\n",
    "            initial_states: initial states.\n",
    "        Returns:\n",
    "            final_states: final states.\n",
    "        \"\"\"\n",
    "        #self.intermediate_states = []\n",
    "        \n",
    "        n_step = us.shape[0]\n",
    "        n_param = us.shape[1]\n",
    "        \n",
    "        if len(Hs) != 2:\n",
    "            print(\"Hs must have 2 elements!\")\n",
    "            \n",
    "        if n_param != 2:\n",
    "            print(\"Num. of parameters must be 2!\")\n",
    "\n",
    "        final_states = initial_states\n",
    "        sgm = torch.nn.Sigmoid()\n",
    "        \n",
    "        for j in range(n_step):\n",
    "            Hjdt = H0.clone()\n",
    "            N = torch.sqrt(torch.sum(torch.square(us[j])))\n",
    "            Hjdt += (2*sgm(N)-1) / N * (us[j][0] * Hs[0].clone() + us[j][1] * Hs[1].clone())\n",
    "            final_states = torch.matmul(torch.matrix_exp(Hjdt), final_states)\n",
    "\n",
    "        return final_states\n",
    "    \n",
    "    \n",
    "    def forward_simulate_ibm2(self, us, H0, Hs, initial_states):\n",
    "        \"\"\"Forward time evolution.\n",
    "        Args:\n",
    "            us: pulses.\n",
    "            H0: a Hermitian matrix.\n",
    "            Hs: a list of Hermitian matrics.\n",
    "            initial_states: initial states.\n",
    "        Returns:\n",
    "            final_states: final states.\n",
    "        \"\"\"\n",
    "        #self.intermediate_states = []\n",
    "\n",
    "        n_step = us.shape[0]\n",
    "        n_param = us.shape[1]\n",
    "        \n",
    "        if len(H0) != 3:\n",
    "            print(\"H0 must have 3 elements!\")\n",
    "        if len(Hs) != 4:\n",
    "            print(\"Hs must have 4 elements!\")\n",
    "            \n",
    "        if n_param != 8:\n",
    "            print(\"Num. of parameters must be 8!\")\n",
    "\n",
    "        final_states = initial_states\n",
    "        sgm = torch.nn.Sigmoid()\n",
    "\n",
    "        for j in range(n_step):\n",
    "            t = j * self.dt\n",
    "            Nd0 = torch.sqrt(torch.sum(torch.square(us[j][0:2])))\n",
    "            Nu0 = torch.sqrt(torch.sum(torch.square(us[j][2:4])))\n",
    "            Nd1 = torch.sqrt(torch.sum(torch.square(us[j][4:6])))\n",
    "            Nu1 = torch.sqrt(torch.sum(torch.square(us[j][6:8])))\n",
    "            Hjdt = H0[0].clone() + np.cos(self.delta*t) * H0[1].clone() + np.sin(self.delta*t) * H0[2].clone()\n",
    "            Hjdt += ((2*sgm(Nd0)-1) / Nd0 * us[j][0]  + (2*sgm(Nu0)-1) / Nu0 *\n",
    "                     (np.cos(self.delta*t)*us[j][2] - np.sin(self.delta*t)*us[j][3])) * Hs[0].clone()\n",
    "            Hjdt += ((2*sgm(Nd0)-1) / Nd0 * us[j][1] + (2*sgm(Nu0)-1) / Nu0 *\n",
    "                     (np.sin(self.delta*t)*us[j][2] + np.cos(self.delta*t)*us[j][3])) * Hs[1].clone()\n",
    "            Hjdt += ((2*sgm(Nd1)-1) / Nd1 * us[j][4] + (2*sgm(Nu1)-1) / Nu1 *\n",
    "                     (np.cos(self.delta*t)*us[j][6] - np.sin(self.delta*t)*us[j][7])) * Hs[2].clone()\n",
    "            Hjdt += ((2*sgm(Nd1)-1) / Nd1 * us[j][5] + (2*sgm(Nu1)-1) / Nu1 *\n",
    "                     (np.sin(self.delta*t)*us[j][6] + np.cos(self.delta*t)*us[j][7])) * Hs[3].clone()\n",
    "            final_states = torch.matmul(torch.matrix_exp(Hjdt), final_states)\n",
    "\n",
    "        return final_states\n",
    "\n",
    "    def save_plot(self, plot_name, us):\n",
    "        return\n",
    "    \n",
    "\n",
    "    def get_inner_product_2D(self,psi1,psi2):\n",
    "        \"\"\"compute the inner product |psi1.dag() * psi2|.\n",
    "        Args:\n",
    "            psi1: a state.\n",
    "            psi2: a psi2.\n",
    "        Returns:\n",
    "            norm: norm of the complex number.\n",
    "        \"\"\"\n",
    "        state_dim = int(psi1.shape[0] / 2)\n",
    "        psi_1_real = psi1[0:state_dim,:]\n",
    "        psi_1_imag = psi1[state_dim:2*state_dim,:]\n",
    "        psi_2_real = psi2[0:state_dim,:]\n",
    "        psi_2_imag = psi2[state_dim:2*state_dim,:]\n",
    "\n",
    "        ac = (psi_1_real * psi_2_real).sum(0) \n",
    "        bd = (psi_1_imag * psi_2_imag).sum(0) \n",
    "        bc = (psi_1_imag * psi_2_real).sum(0) \n",
    "        ad = (psi_1_real * psi_2_imag).sum(0) \n",
    "        reals = (ac + bd).sum()**2\n",
    "        imags = (bc - ad).sum()**2\n",
    "        norm = (reals + imags) / (psi1.shape[1]**2)\n",
    "        return norm\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def c_to_r_mat(M):\n",
    "        # complex to real isomorphism for matrix\n",
    "        return np.asarray(np.bmat([[M.real, -M.imag],[M.imag, M.real]]))\n",
    "\n",
    "    @staticmethod\n",
    "    def c_to_r_vec(V):\n",
    "        # complex to real isomorphism for vector\n",
    "        new_v = []\n",
    "        new_v.append(V.real)\n",
    "        new_v.append(V.imag)\n",
    "        return np.reshape(new_v, [2 * len(V)])\n",
    "        \n",
    "    @staticmethod\n",
    "    def random_initialize_u(n_step, n_Hs):\n",
    "        initial_mean = 0\n",
    "        #initial_stddev =  (1. / np.sqrt(n_step))\n",
    "        initial_stddev = 2\n",
    "        u = np.random.normal(initial_mean, initial_stddev, [n_step ,n_Hs])\n",
    "        \n",
    "        return u\n",
    "    \n",
    "\n",
    "\n",
    "    #====================================================================\n",
    "    def ibm_single_plus(self, duration):\n",
    "        \"\"\"synthesizing the plus state on the IBM machine\n",
    "        \"\"\"\n",
    "        self.logger.write_text(\"ibm_single_plus========\")\n",
    "\n",
    "        n_step = self.n_step\n",
    "        T = duration * 0.22\n",
    "        self.dt = T / self.n_step\n",
    "        w0 = 5236376147.050786 * 2 * np.pi * 1e-9\n",
    "        eps0 = 32901013497.991684 * 1e-9\n",
    "        Omega0 = 955111374.7779446 * 1e-9\n",
    "        Delta0 = eps0 - w0\n",
    "\n",
    "        I = np.array([[1, 0], \n",
    "                    [0, 1]])\n",
    "        X = np.array([[0, 1], \n",
    "                    [1, 0]])\n",
    "        Y = (0+1j) * np.array([[0, -1], \n",
    "                            [1, 0]])\n",
    "        Z = np.array([[1, 0], \n",
    "                    [0, -1]])\n",
    "        \n",
    "        # apply RWA\n",
    "        H0 = 0.5 * Delta0 * (I-Z)\n",
    "        Hs = [0.5 * Omega0 * X, 0.5 * Omega0 * Y]\n",
    "        \n",
    "        Udag0 = np.array([[1, 0], \n",
    "                    [0, np.exp(-1.j * w0 * T)]])\n",
    "\n",
    "        g = np.array([1., 0.])\n",
    "        e = np.array([0., 1.])\n",
    "        plus = 1/np.sqrt(2) * (g+e)\n",
    "        \n",
    "        psi_in = [g]\n",
    "        psi_out = [Udag0 @ plus]\n",
    "\n",
    "        target_states = [self.c_to_r_vec(v) for v in psi_out]\n",
    "        initial_states = [self.c_to_r_vec(v) for v in psi_in]\n",
    "\n",
    "        init_u = self.random_initialize_u(n_step, len(Hs))\n",
    "        final_u = self.train_fidelity_ibm1(init_u, H0, Hs, initial_states, target_states)\n",
    "    \n",
    "    \n",
    "    def ibm_X(self, duration):\n",
    "        \"\"\"synthesizing the plus state on the IBM machine\n",
    "        \"\"\"\n",
    "        self.logger.write_text(\"ibm_X========\")\n",
    "\n",
    "        n_step = self.n_step\n",
    "        T = duration * 0.22\n",
    "        self.dt = T / self.n_step\n",
    "        w0 = 5236376147.050786 * 2 * np.pi * 1e-9\n",
    "        eps0 = 32901013497.991684 * 1e-9\n",
    "        Omega0 = 955111374.7779446 * 1e-9\n",
    "        Delta0 = eps0 - w0\n",
    "\n",
    "        I = np.array([[1, 0], \n",
    "                    [0, 1]])\n",
    "        X = np.array([[0, 1], \n",
    "                    [1, 0]])\n",
    "        Y = (0+1j) * np.array([[0, -1], \n",
    "                            [1, 0]])\n",
    "        Z = np.array([[1, 0], \n",
    "                    [0, -1]])\n",
    "        \n",
    "        # apply RWA\n",
    "        H0 = 0.5 * Delta0 * (I-Z)\n",
    "        Hs = [0.5 * Omega0 * X, 0.5 * Omega0 * Y]\n",
    "        \n",
    "        Udag0 = np.array([[1, 0], \n",
    "                    [0, np.exp(-1.j * w0 * T)]])\n",
    "\n",
    "        g = np.array([1., 0.])\n",
    "        e = np.array([0., 1.])\n",
    "        plus = 1/np.sqrt(2) * (g+e)\n",
    "        \n",
    "        psi_in = [g, e, plus]\n",
    "        psi_out = [Udag0 @ e, Udag0 @ g, Udag0 @ plus]\n",
    "\n",
    "        target_states = [self.c_to_r_vec(v) for v in psi_out]\n",
    "        initial_states = [self.c_to_r_vec(v) for v in psi_in]\n",
    "\n",
    "        init_u = self.random_initialize_u(n_step, len(Hs))\n",
    "        final_u = self.train_fidelity_ibm1(init_u, H0, Hs, initial_states, target_states)\n",
    "    \n",
    "        \n",
    "        \n",
    "    #====================================================================\n",
    "    def ibm_bell_state(self, duration):\n",
    "        \"\"\"synthesizing the X gate on the IBM machine\n",
    "        \"\"\"\n",
    "        self.logger.write_text(\"ibm_bell_state ========\")\n",
    "\n",
    "        n_step = self.n_step\n",
    "        T = duration * 0.22\n",
    "        self.dt = T / self.n_step\n",
    "        w0 = 5236376147.050786 * 2 * np.pi * 1e-9\n",
    "        w1 = 5014084426.228487 * 2 * np.pi * 1e-9\n",
    "        self.delta = w1 - w0\n",
    "        eps0 = 32901013497.991684 * 1e-9\n",
    "        eps1 = 31504959831.439907 * 1e-9\n",
    "        Omega0 = 955111374.7779446 * 1e-9\n",
    "        Omega1 = 987150040.8532522 * 1e-9\n",
    "        Delta0 = eps0 - w0\n",
    "        Delta1 = eps1 - w1\n",
    "        j01 = 12286377.631357463 * 1e-9\n",
    "\n",
    "        I = np.array([[1, 0], \n",
    "                    [0, 1]])\n",
    "        X = np.array([[0, 1], \n",
    "                    [1, 0]])\n",
    "        Y = (0+1j) * np.array([[0, -1], \n",
    "                            [1, 0]])\n",
    "        Z = np.array([[1, 0], \n",
    "                    [0, -1]])\n",
    "        \n",
    "        Hint_re = np.array([[0,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,0]])\n",
    "        Hint_im = 1.j* np.array([[0,0,0,0],[0,0,-1,0],[0,1,0,0],[0,0,0,0]])\n",
    "        \n",
    "        #U_to_learn = X\n",
    "        #print(U_to_learn)\n",
    "\n",
    "        # apply RWA\n",
    "        Hsys_1q = 0.5 * (I-Z)\n",
    "        Id = np.eye(2)\n",
    "        H0 = [Delta0 * kron(Hsys_1q,Id)+Delta1 * kron(Id, Hsys_1q), \n",
    "              j01*Hint_re, j01*Hint_im]\n",
    "        Hs = [0.5 * Omega0 * kron(X,Id), 0.5 * Omega0 * kron(Y,Id),\n",
    "              0.5 * Omega1 * kron(Id,X), 0.5 * Omega1 * kron(Id,Y)]\n",
    "        \n",
    "        Udag0 = np.array([[1, 0], \n",
    "                    [0, np.exp(-1.j * w0 * T)]])\n",
    "        Udag1 = np.array([[1, 0], \n",
    "                    [0, np.exp(-1.j * w1 * T)]])\n",
    "        Udag = kron(Udag0, Udag1)\n",
    "\n",
    "        g = np.array([1., 0.])\n",
    "        e = np.array([0., 1.])\n",
    "        plus = 1/np.sqrt(2) * (g+e)\n",
    "        state_in = kron(g,g)\n",
    "        state_out = 1/np.sqrt(2) * (kron(g,g)+kron(e,e))\n",
    "        psi_in = [state_in]\n",
    "        psi_out = [Udag @ state_out]\n",
    "\n",
    "        target_states = [self.c_to_r_vec(v) for v in psi_out]\n",
    "        initial_states = [self.c_to_r_vec(v) for v in psi_in]\n",
    "\n",
    "        init_u = self.random_initialize_u(n_step, 2*len(Hs))  \n",
    "        final_u = self.train_fidelity_ibm2(init_u, H0, Hs, initial_states, target_states)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def ibm_CNOT(self, duration):\n",
    "        \"\"\"synthesizing the X gate on the IBM machine\n",
    "        \"\"\"\n",
    "        self.logger.write_text(\"ibm_CNOT ========\")\n",
    "\n",
    "        n_step = self.n_step\n",
    "        T = duration * 0.22\n",
    "        self.dt = T / self.n_step\n",
    "        w0 = 5236376147.050786 * 2 * np.pi * 1e-9\n",
    "        w1 = 5014084426.228487 * 2 * np.pi * 1e-9\n",
    "        self.delta = w1 - w0\n",
    "        eps0 = 32901013497.991684 * 1e-9\n",
    "        eps1 = 31504959831.439907 * 1e-9\n",
    "        Omega0 = 955111374.7779446 * 1e-9\n",
    "        Omega1 = 987150040.8532522 * 1e-9\n",
    "        Delta0 = eps0 - w0\n",
    "        Delta1 = eps1 - w1\n",
    "        j01 = 12286377.631357463 * 1e-9\n",
    "\n",
    "        I = np.array([[1, 0], \n",
    "                    [0, 1]])\n",
    "        X = np.array([[0, 1], \n",
    "                    [1, 0]])\n",
    "        Y = (0+1j) * np.array([[0, -1], \n",
    "                            [1, 0]])\n",
    "        Z = np.array([[1, 0], \n",
    "                    [0, -1]])\n",
    "        \n",
    "        Hint_re = np.array([[0,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,0]])\n",
    "        Hint_im = 1.j* np.array([[0,0,0,0],[0,0,-1,0],[0,1,0,0],[0,0,0,0]])\n",
    "        \n",
    "        #U_to_learn = X\n",
    "        #print(U_to_learn)\n",
    "\n",
    "        # apply RWA\n",
    "        Hsys_1q = 0.5 * (I-Z)\n",
    "        Id = np.eye(2)\n",
    "        H0 = [Delta0 * kron(Hsys_1q,Id)+Delta1 * kron(Id, Hsys_1q), \n",
    "              j01*Hint_re, j01*Hint_im]\n",
    "        Hs = [0.5 * Omega0 * kron(X,Id), 0.5 * Omega0 * kron(Y,Id),\n",
    "              0.5 * Omega1 * kron(Id,X), 0.5 * Omega1 * kron(Id,Y)]\n",
    "        \n",
    "        Udag0 = np.array([[1, 0], \n",
    "                    [0, np.exp(-1.j * w0 * T)]])\n",
    "        Udag1 = np.array([[1, 0], \n",
    "                    [0, np.exp(-1.j * w1 * T)]])\n",
    "        Udag = kron(Udag0, Udag1)\n",
    "\n",
    "        g = np.array([1., 0.])\n",
    "        e = np.array([0., 1.])\n",
    "        plus = 1/np.sqrt(2) * (g+e)\n",
    "        state_in1 = kron(g,g)\n",
    "        state_in2 = kron(g,e)\n",
    "        state_in3 = kron(e,g)\n",
    "        state_in4 = kron(e,e)\n",
    "        state_in5 = kron(plus,plus)\n",
    "        psi_in = [state_in1, state_in2, state_in3, state_in4, state_in5]\n",
    "        psi_out = [Udag @ state_in1, Udag @ state_in2, Udag @ state_in4, Udag @ state_in3, Udag @ state_in5]\n",
    "\n",
    "        target_states = [self.c_to_r_vec(v) for v in psi_out]\n",
    "        initial_states = [self.c_to_r_vec(v) for v in psi_in]\n",
    "\n",
    "        init_u = self.random_initialize_u(n_step, 2*len(Hs))\n",
    "        final_u = self.train_fidelity_ibm2(init_u, H0, Hs, initial_states, target_states)\n",
    "\n",
    "        #plt.loglog(self.losses_energy)\n",
    "        #plt.savefig(\"grape.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59356843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs are written to ./logs/text/20220120_145329.txt\n",
      "arguments ========\n",
      "taylor_terms: 20\n",
      "n_step: 100\n",
      "n_epoch: 1200\n",
      "lr: 0.01\n",
      "ibm_CNOT ========\n",
      "!!!! train_fidelity ========\n",
      "epoch: 0000, loss: 0.9878234659247654, loss_fid: 0.9878234659247654\n",
      "epoch: 0001, loss: 0.7930937290810443, loss_fid: 0.7930937290810443\n",
      "epoch: 0002, loss: 0.5605988064723859, loss_fid: 0.5605988064723859\n",
      "epoch: 0003, loss: 0.4676872166486058, loss_fid: 0.4676872166486058\n",
      "epoch: 0004, loss: 0.5635598748983952, loss_fid: 0.5635598748983952\n",
      "epoch: 0005, loss: 0.5854343075050650, loss_fid: 0.5854343075050650\n",
      "epoch: 0006, loss: 0.5279064149060311, loss_fid: 0.5279064149060311\n",
      "epoch: 0007, loss: 0.4543136360970041, loss_fid: 0.4543136360970041\n",
      "epoch: 0008, loss: 0.4122841383573788, loss_fid: 0.4122841383573788\n",
      "epoch: 0009, loss: 0.4326790415373634, loss_fid: 0.4326790415373634\n",
      "epoch: 0010, loss: 0.4617616503662105, loss_fid: 0.4617616503662105\n",
      "epoch: 0011, loss: 0.4559851357981310, loss_fid: 0.4559851357981310\n",
      "epoch: 0012, loss: 0.4219971030499778, loss_fid: 0.4219971030499778\n",
      "epoch: 0013, loss: 0.3854847360462625, loss_fid: 0.3854847360462625\n",
      "epoch: 0014, loss: 0.3699581591545111, loss_fid: 0.3699581591545111\n",
      "epoch: 0015, loss: 0.3807809481161815, loss_fid: 0.3807809481161815\n",
      "epoch: 0016, loss: 0.3944150917616348, loss_fid: 0.3944150917616348\n",
      "epoch: 0017, loss: 0.3883886933210302, loss_fid: 0.3883886933210302\n",
      "epoch: 0018, loss: 0.3678657336407545, loss_fid: 0.3678657336407545\n",
      "epoch: 0019, loss: 0.3459530791059335, loss_fid: 0.3459530791059335\n",
      "epoch: 0020, loss: 0.3360900055119344, loss_fid: 0.3360900055119344\n",
      "epoch: 0021, loss: 0.3417634758802996, loss_fid: 0.3417634758802996\n",
      "epoch: 0022, loss: 0.3464885007310915, loss_fid: 0.3464885007310915\n",
      "epoch: 0023, loss: 0.3404433089379165, loss_fid: 0.3404433089379165\n",
      "epoch: 0024, loss: 0.3255422929100473, loss_fid: 0.3255422929100473\n",
      "epoch: 0025, loss: 0.3113230238610123, loss_fid: 0.3113230238610123\n",
      "epoch: 0026, loss: 0.3073091736713934, loss_fid: 0.3073091736713934\n",
      "epoch: 0027, loss: 0.3089813874967706, loss_fid: 0.3089813874967706\n",
      "epoch: 0028, loss: 0.3087153355418298, loss_fid: 0.3087153355418298\n",
      "epoch: 0029, loss: 0.3017371968846954, loss_fid: 0.3017371968846954\n",
      "epoch: 0030, loss: 0.2905184074684981, loss_fid: 0.2905184074684981\n",
      "epoch: 0031, loss: 0.2833449318112620, loss_fid: 0.2833449318112620\n",
      "epoch: 0032, loss: 0.2820139251823557, loss_fid: 0.2820139251823557\n",
      "epoch: 0033, loss: 0.2825684627816056, loss_fid: 0.2825684627816056\n",
      "epoch: 0034, loss: 0.2796185134698463, loss_fid: 0.2796185134698463\n",
      "epoch: 0035, loss: 0.2725981533515031, loss_fid: 0.2725981533515031\n",
      "epoch: 0036, loss: 0.2665004111747846, loss_fid: 0.2665004111747846\n",
      "epoch: 0037, loss: 0.2640630663849709, loss_fid: 0.2640630663849709\n",
      "epoch: 0038, loss: 0.2640601152403168, loss_fid: 0.2640601152403168\n",
      "epoch: 0039, loss: 0.2619844767217621, loss_fid: 0.2619844767217621\n",
      "epoch: 0040, loss: 0.2572716657538905, loss_fid: 0.2572716657538905\n",
      "epoch: 0041, loss: 0.2525875629136678, loss_fid: 0.2525875629136678\n",
      "epoch: 0042, loss: 0.2504572860625514, loss_fid: 0.2504572860625514\n",
      "epoch: 0043, loss: 0.2498542222025459, loss_fid: 0.2498542222025459\n",
      "epoch: 0044, loss: 0.2477278983229831, loss_fid: 0.2477278983229831\n",
      "epoch: 0045, loss: 0.2438057848729160, loss_fid: 0.2438057848729160\n",
      "epoch: 0046, loss: 0.2400923382667151, loss_fid: 0.2400923382667151\n",
      "epoch: 0047, loss: 0.2383497586294133, loss_fid: 0.2383497586294133\n",
      "epoch: 0048, loss: 0.2370213336242418, loss_fid: 0.2370213336242418\n",
      "epoch: 0049, loss: 0.2345449658798749, loss_fid: 0.2345449658798749\n",
      "epoch: 0050, loss: 0.2310788350294958, loss_fid: 0.2310788350294958\n",
      "epoch: 0051, loss: 0.2284880050286507, loss_fid: 0.2284880050286507\n",
      "epoch: 0052, loss: 0.2269214072465744, loss_fid: 0.2269214072465744\n",
      "epoch: 0053, loss: 0.2252603123151425, loss_fid: 0.2252603123151425\n",
      "epoch: 0054, loss: 0.2225821635894550, loss_fid: 0.2225821635894550\n",
      "epoch: 0055, loss: 0.2199080440981127, loss_fid: 0.2199080440981127\n",
      "epoch: 0056, loss: 0.2180209865490341, loss_fid: 0.2180209865490341\n",
      "epoch: 0057, loss: 0.2164840952333720, loss_fid: 0.2164840952333720\n",
      "epoch: 0058, loss: 0.2143567912139390, loss_fid: 0.2143567912139390\n",
      "epoch: 0059, loss: 0.2118692902212627, loss_fid: 0.2118692902212627\n",
      "epoch: 0060, loss: 0.2097930150216786, loss_fid: 0.2097930150216786\n",
      "epoch: 0061, loss: 0.2081683684185645, loss_fid: 0.2081683684185645\n",
      "epoch: 0062, loss: 0.2062972826776988, loss_fid: 0.2062972826776988\n",
      "epoch: 0063, loss: 0.2040216476076618, loss_fid: 0.2040216476076618\n",
      "epoch: 0064, loss: 0.2019533674384845, loss_fid: 0.2019533674384845\n",
      "epoch: 0065, loss: 0.2002316439769963, loss_fid: 0.2002316439769963\n",
      "epoch: 0066, loss: 0.1984464965008987, loss_fid: 0.1984464965008987\n",
      "epoch: 0067, loss: 0.1963496007764230, loss_fid: 0.1963496007764230\n",
      "epoch: 0068, loss: 0.1943582719239169, loss_fid: 0.1943582719239169\n",
      "epoch: 0069, loss: 0.1926392562040478, loss_fid: 0.1926392562040478\n",
      "epoch: 0070, loss: 0.1908956676152940, loss_fid: 0.1908956676152940\n",
      "epoch: 0071, loss: 0.1889565043957692, loss_fid: 0.1889565043957692\n",
      "epoch: 0072, loss: 0.1870759116671695, loss_fid: 0.1870759116671695\n",
      "epoch: 0073, loss: 0.1854107656188200, loss_fid: 0.1854107656188200\n",
      "epoch: 0074, loss: 0.1837155850572566, loss_fid: 0.1837155850572566\n",
      "epoch: 0075, loss: 0.1819031501888685, loss_fid: 0.1819031501888685\n",
      "epoch: 0076, loss: 0.1801540525645990, loss_fid: 0.1801540525645990\n",
      "epoch: 0077, loss: 0.1785483362556206, loss_fid: 0.1785483362556206\n",
      "epoch: 0078, loss: 0.1769171560485043, loss_fid: 0.1769171560485043\n",
      "epoch: 0079, loss: 0.1752059843643127, loss_fid: 0.1752059843643127\n",
      "epoch: 0080, loss: 0.1735760820255274, loss_fid: 0.1735760820255274\n",
      "epoch: 0081, loss: 0.1720350539666745, loss_fid: 0.1720350539666745\n",
      "epoch: 0082, loss: 0.1704562775602271, loss_fid: 0.1704562775602271\n",
      "epoch: 0083, loss: 0.1688511951023633, loss_fid: 0.1688511951023633\n",
      "epoch: 0084, loss: 0.1673198794835687, loss_fid: 0.1673198794835687\n",
      "epoch: 0085, loss: 0.1658356150537018, loss_fid: 0.1658356150537018\n",
      "epoch: 0086, loss: 0.1643127296151762, loss_fid: 0.1643127296151762\n",
      "epoch: 0087, loss: 0.1627990487187736, loss_fid: 0.1627990487187736\n",
      "epoch: 0088, loss: 0.1613536830236910, loss_fid: 0.1613536830236910\n",
      "epoch: 0089, loss: 0.1599178081999389, loss_fid: 0.1599178081999389\n",
      "epoch: 0090, loss: 0.1584636291041944, loss_fid: 0.1584636291041944\n",
      "epoch: 0091, loss: 0.1570460545663050, loss_fid: 0.1570460545663050\n",
      "epoch: 0092, loss: 0.1556697825660174, loss_fid: 0.1556697825660174\n",
      "epoch: 0093, loss: 0.1542880476730778, loss_fid: 0.1542880476730778\n",
      "epoch: 0094, loss: 0.1529114484699412, loss_fid: 0.1529114484699412\n",
      "epoch: 0095, loss: 0.1515748151543835, loss_fid: 0.1515748151543835\n",
      "epoch: 0096, loss: 0.1502565581379710, loss_fid: 0.1502565581379710\n",
      "epoch: 0097, loss: 0.1489342451272968, loss_fid: 0.1489342451272968\n",
      "epoch: 0098, loss: 0.1476365123912138, loss_fid: 0.1476365123912138\n",
      "epoch: 0099, loss: 0.1463681204450714, loss_fid: 0.1463681204450714\n",
      "epoch: 0100, loss: 0.1451010959592878, loss_fid: 0.1451010959592878\n",
      "epoch: 0101, loss: 0.1438448636904280, loss_fid: 0.1438448636904280\n",
      "epoch: 0102, loss: 0.1426178193669949, loss_fid: 0.1426178193669949\n",
      "epoch: 0103, loss: 0.1414026382058942, loss_fid: 0.1414026382058942\n",
      "epoch: 0104, loss: 0.1401935500136193, loss_fid: 0.1401935500136193\n",
      "epoch: 0105, loss: 0.1390077761684207, loss_fid: 0.1390077761684207\n",
      "epoch: 0106, loss: 0.1378393743632862, loss_fid: 0.1378393743632862\n",
      "epoch: 0107, loss: 0.1366765247289439, loss_fid: 0.1366765247289439\n",
      "epoch: 0108, loss: 0.1355308430890116, loss_fid: 0.1355308430890116\n",
      "epoch: 0109, loss: 0.1344048924929231, loss_fid: 0.1344048924929231\n",
      "epoch: 0110, loss: 0.1332878383639837, loss_fid: 0.1332878383639837\n",
      "epoch: 0111, loss: 0.1321844353229222, loss_fid: 0.1321844353229222\n",
      "epoch: 0112, loss: 0.1311003810457692, loss_fid: 0.1311003810457692\n",
      "epoch: 0113, loss: 0.1300271481933205, loss_fid: 0.1300271481933205\n",
      "epoch: 0114, loss: 0.1289653484601463, loss_fid: 0.1289653484601463\n",
      "epoch: 0115, loss: 0.1279214433494468, loss_fid: 0.1279214433494468\n",
      "epoch: 0116, loss: 0.1268906175429143, loss_fid: 0.1268906175429143\n",
      "epoch: 0117, loss: 0.1258713386166674, loss_fid: 0.1258713386166674\n",
      "epoch: 0118, loss: 0.1248691185462392, loss_fid: 0.1248691185462392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0119, loss: 0.1238825869738608, loss_fid: 0.1238825869738608\n",
      "epoch: 0120, loss: 0.1229136525162804, loss_fid: 0.1229136525162804\n",
      "epoch: 0121, loss: 0.1219795212232014, loss_fid: 0.1219795212232014\n",
      "epoch: 0122, loss: 0.1211197231602767, loss_fid: 0.1211197231602767\n",
      "epoch: 0123, loss: 0.1204511403562870, loss_fid: 0.1204511403562870\n",
      "epoch: 0124, loss: 0.1202900264870798, loss_fid: 0.1202900264870798\n",
      "epoch: 0125, loss: 0.1210556274710797, loss_fid: 0.1210556274710797\n",
      "epoch: 0126, loss: 0.1219910302324664, loss_fid: 0.1219910302324664\n",
      "epoch: 0127, loss: 0.1198512364540365, loss_fid: 0.1198512364540365\n",
      "epoch: 0128, loss: 0.1161046577766897, loss_fid: 0.1161046577766897\n",
      "epoch: 0129, loss: 0.1159742276564185, loss_fid: 0.1159742276564185\n",
      "epoch: 0130, loss: 0.1168322026442786, loss_fid: 0.1168322026442786\n",
      "epoch: 0131, loss: 0.1145656060594287, loss_fid: 0.1145656060594287\n",
      "epoch: 0132, loss: 0.1130240870900456, loss_fid: 0.1130240870900456\n",
      "epoch: 0133, loss: 0.1135528388064014, loss_fid: 0.1135528388064014\n",
      "epoch: 0134, loss: 0.1122586474239760, loss_fid: 0.1122586474239760\n",
      "epoch: 0135, loss: 0.1108337913326242, loss_fid: 0.1108337913326242\n",
      "epoch: 0136, loss: 0.1109067923699679, loss_fid: 0.1109067923699679\n",
      "epoch: 0137, loss: 0.1098252762359262, loss_fid: 0.1098252762359262\n",
      "epoch: 0138, loss: 0.1087204345113468, loss_fid: 0.1087204345113468\n",
      "epoch: 0139, loss: 0.1086223309146217, loss_fid: 0.1086223309146217\n",
      "epoch: 0140, loss: 0.1075574684317627, loss_fid: 0.1075574684317627\n",
      "epoch: 0141, loss: 0.1066544739958989, loss_fid: 0.1066544739958989\n",
      "epoch: 0142, loss: 0.1064880275993942, loss_fid: 0.1064880275993942\n",
      "epoch: 0143, loss: 0.1054822066337476, loss_fid: 0.1054822066337476\n",
      "epoch: 0144, loss: 0.1046872503398418, loss_fid: 0.1046872503398418\n",
      "epoch: 0145, loss: 0.1044315395037835, loss_fid: 0.1044315395037835\n",
      "epoch: 0146, loss: 0.1035407306551555, loss_fid: 0.1035407306551555\n",
      "epoch: 0147, loss: 0.1028349409911398, loss_fid: 0.1028349409911398\n",
      "epoch: 0148, loss: 0.1024645866784013, loss_fid: 0.1024645866784013\n",
      "epoch: 0149, loss: 0.1016730072420168, loss_fid: 0.1016730072420168\n",
      "epoch: 0150, loss: 0.1010681605778792, loss_fid: 0.1010681605778792\n",
      "epoch: 0151, loss: 0.1006124820114878, loss_fid: 0.1006124820114878\n",
      "epoch: 0152, loss: 0.0998686557325834, loss_fid: 0.0998686557325834\n",
      "epoch: 0153, loss: 0.0993384255532297, loss_fid: 0.0993384255532297\n",
      "epoch: 0154, loss: 0.0988646130996395, loss_fid: 0.0988646130996395\n",
      "epoch: 0155, loss: 0.0981509036186106, loss_fid: 0.0981509036186106\n",
      "epoch: 0156, loss: 0.0976361616897782, loss_fid: 0.0976361616897782\n",
      "epoch: 0157, loss: 0.0971765244132194, loss_fid: 0.0971765244132194\n",
      "epoch: 0158, loss: 0.0965142309557322, loss_fid: 0.0965142309557322\n",
      "epoch: 0159, loss: 0.0959916388926685, loss_fid: 0.0959916388926685\n",
      "epoch: 0160, loss: 0.0955339365129955, loss_fid: 0.0955339365129955\n",
      "epoch: 0161, loss: 0.0949228038971832, loss_fid: 0.0949228038971832\n",
      "epoch: 0162, loss: 0.0944034209724833, loss_fid: 0.0944034209724833\n",
      "epoch: 0163, loss: 0.0939521398313672, loss_fid: 0.0939521398313672\n",
      "epoch: 0164, loss: 0.0933779637057013, loss_fid: 0.0933779637057013\n",
      "epoch: 0165, loss: 0.0928517545633677, loss_fid: 0.0928517545633677\n",
      "epoch: 0166, loss: 0.0924105034056069, loss_fid: 0.0924105034056069\n",
      "epoch: 0167, loss: 0.0918852920787845, loss_fid: 0.0918852920787845\n",
      "epoch: 0168, loss: 0.0913561248677692, loss_fid: 0.0913561248677692\n",
      "epoch: 0169, loss: 0.0909036775524344, loss_fid: 0.0909036775524344\n",
      "epoch: 0170, loss: 0.0904169284878189, loss_fid: 0.0904169284878189\n",
      "epoch: 0171, loss: 0.0899062705187190, loss_fid: 0.0899062705187190\n",
      "epoch: 0172, loss: 0.0894482422172084, loss_fid: 0.0894482422172084\n",
      "epoch: 0173, loss: 0.0889839579419367, loss_fid: 0.0889839579419367\n",
      "epoch: 0174, loss: 0.0884903796704927, loss_fid: 0.0884903796704927\n",
      "epoch: 0175, loss: 0.0880283134894501, loss_fid: 0.0880283134894501\n",
      "epoch: 0176, loss: 0.0875808561532128, loss_fid: 0.0875808561532128\n",
      "epoch: 0177, loss: 0.0871110567585626, loss_fid: 0.0871110567585626\n",
      "epoch: 0178, loss: 0.0866520171569326, loss_fid: 0.0866520171569326\n",
      "epoch: 0179, loss: 0.0862122269095393, loss_fid: 0.0862122269095393\n",
      "epoch: 0180, loss: 0.0857597846367203, loss_fid: 0.0857597846367203\n",
      "epoch: 0181, loss: 0.0853060080739408, loss_fid: 0.0853060080739408\n",
      "epoch: 0182, loss: 0.0848727772107732, loss_fid: 0.0848727772107732\n",
      "epoch: 0183, loss: 0.0844400481711278, loss_fid: 0.0844400481711278\n",
      "epoch: 0184, loss: 0.0839976868765672, loss_fid: 0.0839976868765672\n",
      "epoch: 0185, loss: 0.0835661780933782, loss_fid: 0.0835661780933782\n",
      "epoch: 0186, loss: 0.0831459175939867, loss_fid: 0.0831459175939867\n",
      "epoch: 0187, loss: 0.0827193879131214, loss_fid: 0.0827193879131214\n",
      "epoch: 0188, loss: 0.0822915934331472, loss_fid: 0.0822915934331472\n",
      "epoch: 0189, loss: 0.0818756294804700, loss_fid: 0.0818756294804700\n",
      "epoch: 0190, loss: 0.0814644034536310, loss_fid: 0.0814644034536310\n",
      "epoch: 0191, loss: 0.0810492639464907, loss_fid: 0.0810492639464907\n",
      "epoch: 0192, loss: 0.0806374489377130, loss_fid: 0.0806374489377130\n",
      "epoch: 0193, loss: 0.0802339754053333, loss_fid: 0.0802339754053333\n",
      "epoch: 0194, loss: 0.0798323076733412, loss_fid: 0.0798323076733412\n",
      "epoch: 0195, loss: 0.0794298300590771, loss_fid: 0.0794298300590771\n",
      "epoch: 0196, loss: 0.0790317658486696, loss_fid: 0.0790317658486696\n",
      "epoch: 0197, loss: 0.0786391264771231, loss_fid: 0.0786391264771231\n",
      "epoch: 0198, loss: 0.0782477716383991, loss_fid: 0.0782477716383991\n",
      "epoch: 0199, loss: 0.0778575761365287, loss_fid: 0.0778575761365287\n",
      "epoch: 0200, loss: 0.0774722224987860, loss_fid: 0.0774722224987860\n",
      "epoch: 0201, loss: 0.0770918235165265, loss_fid: 0.0770918235165265\n",
      "epoch: 0202, loss: 0.0767140139600576, loss_fid: 0.0767140139600576\n",
      "epoch: 0203, loss: 0.0763403870829632, loss_fid: 0.0763403870829632\n",
      "epoch: 0204, loss: 0.0759759015303064, loss_fid: 0.0759759015303064\n",
      "epoch: 0205, loss: 0.0756257474786398, loss_fid: 0.0756257474786398\n",
      "epoch: 0206, loss: 0.0752986413360610, loss_fid: 0.0752986413360610\n",
      "epoch: 0207, loss: 0.0750160035787417, loss_fid: 0.0750160035787417\n",
      "epoch: 0208, loss: 0.0748230800365209, loss_fid: 0.0748230800365209\n",
      "epoch: 0209, loss: 0.0747983316857198, loss_fid: 0.0747983316857198\n",
      "epoch: 0210, loss: 0.0750488104620795, loss_fid: 0.0750488104620795\n",
      "epoch: 0211, loss: 0.0755971541997782, loss_fid: 0.0755971541997782\n",
      "epoch: 0212, loss: 0.0760910042597810, loss_fid: 0.0760910042597810\n",
      "epoch: 0213, loss: 0.0756334543745248, loss_fid: 0.0756334543745248\n",
      "epoch: 0214, loss: 0.0739485695000742, loss_fid: 0.0739485695000742\n",
      "epoch: 0215, loss: 0.0722899721972702, loss_fid: 0.0722899721972702\n",
      "epoch: 0216, loss: 0.0719727015927214, loss_fid: 0.0719727015927214\n",
      "epoch: 0217, loss: 0.0725394870499234, loss_fid: 0.0725394870499234\n",
      "epoch: 0218, loss: 0.0725906660113955, loss_fid: 0.0725906660113955\n",
      "epoch: 0219, loss: 0.0716559095388069, loss_fid: 0.0716559095388069\n",
      "epoch: 0220, loss: 0.0706424668633520, loss_fid: 0.0706424668633520\n",
      "epoch: 0221, loss: 0.0703836038543804, loss_fid: 0.0703836038543804\n",
      "epoch: 0222, loss: 0.0705018194399734, loss_fid: 0.0705018194399734\n",
      "epoch: 0223, loss: 0.0702479776885676, loss_fid: 0.0702479776885676\n",
      "epoch: 0224, loss: 0.0696021669829449, loss_fid: 0.0696021669829449\n",
      "epoch: 0225, loss: 0.0690584876942489, loss_fid: 0.0690584876942489\n",
      "epoch: 0226, loss: 0.0688134805403859, loss_fid: 0.0688134805403859\n",
      "epoch: 0227, loss: 0.0686366001302035, loss_fid: 0.0686366001302035\n",
      "epoch: 0228, loss: 0.0683323733819166, loss_fid: 0.0683323733819166\n",
      "epoch: 0229, loss: 0.0679282910779526, loss_fid: 0.0679282910779526\n",
      "epoch: 0230, loss: 0.0675170704142690, loss_fid: 0.0675170704142690\n",
      "epoch: 0231, loss: 0.0671792283045414, loss_fid: 0.0671792283045414\n",
      "epoch: 0232, loss: 0.0669445451754612, loss_fid: 0.0669445451754612\n",
      "epoch: 0233, loss: 0.0667249368215462, loss_fid: 0.0667249368215462\n",
      "epoch: 0234, loss: 0.0663893695877416, loss_fid: 0.0663893695877416\n",
      "epoch: 0235, loss: 0.0659618028677703, loss_fid: 0.0659618028677703\n",
      "epoch: 0236, loss: 0.0656232174460613, loss_fid: 0.0656232174460613\n",
      "epoch: 0237, loss: 0.0654335891232231, loss_fid: 0.0654335891232231\n",
      "epoch: 0238, loss: 0.0652311682700527, loss_fid: 0.0652311682700527\n",
      "epoch: 0239, loss: 0.0648827027194286, loss_fid: 0.0648827027194286\n",
      "epoch: 0240, loss: 0.0644807027926465, loss_fid: 0.0644807027926465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0241, loss: 0.0641890528479737, loss_fid: 0.0641890528479737\n",
      "epoch: 0242, loss: 0.0639986326049491, loss_fid: 0.0639986326049491\n",
      "epoch: 0243, loss: 0.0637685502797470, loss_fid: 0.0637685502797470\n",
      "epoch: 0244, loss: 0.0634416860970544, loss_fid: 0.0634416860970544\n",
      "epoch: 0245, loss: 0.0631012913880723, loss_fid: 0.0631012913880723\n",
      "epoch: 0246, loss: 0.0628273657069759, loss_fid: 0.0628273657069759\n",
      "epoch: 0247, loss: 0.0625973550498710, loss_fid: 0.0625973550498710\n",
      "epoch: 0248, loss: 0.0623484894694961, loss_fid: 0.0623484894694961\n",
      "epoch: 0249, loss: 0.0620667554022519, loss_fid: 0.0620667554022519\n",
      "epoch: 0250, loss: 0.0617826794777724, loss_fid: 0.0617826794777724\n",
      "epoch: 0251, loss: 0.0615145278222177, loss_fid: 0.0615145278222177\n",
      "epoch: 0252, loss: 0.0612526998518298, loss_fid: 0.0612526998518298\n",
      "epoch: 0253, loss: 0.0609887866262631, loss_fid: 0.0609887866262631\n",
      "epoch: 0254, loss: 0.0607296570971830, loss_fid: 0.0607296570971830\n",
      "epoch: 0255, loss: 0.0604809905812523, loss_fid: 0.0604809905812523\n",
      "epoch: 0256, loss: 0.0602338098898270, loss_fid: 0.0602338098898270\n",
      "epoch: 0257, loss: 0.0599757334893313, loss_fid: 0.0599757334893313\n",
      "epoch: 0258, loss: 0.0597080531111261, loss_fid: 0.0597080531111261\n",
      "epoch: 0259, loss: 0.0594444962118723, loss_fid: 0.0594444962118723\n",
      "epoch: 0260, loss: 0.0591954511115383, loss_fid: 0.0591954511115383\n",
      "epoch: 0261, loss: 0.0589576659557145, loss_fid: 0.0589576659557145\n",
      "epoch: 0262, loss: 0.0587198851566402, loss_fid: 0.0587198851566402\n",
      "epoch: 0263, loss: 0.0584750664782223, loss_fid: 0.0584750664782223\n",
      "epoch: 0264, loss: 0.0582251452261963, loss_fid: 0.0582251452261963\n",
      "epoch: 0265, loss: 0.0579763674326835, loss_fid: 0.0579763674326835\n",
      "epoch: 0266, loss: 0.0577323745249341, loss_fid: 0.0577323745249341\n",
      "epoch: 0267, loss: 0.0574919072210019, loss_fid: 0.0574919072210019\n",
      "epoch: 0268, loss: 0.0572522788724568, loss_fid: 0.0572522788724568\n",
      "epoch: 0269, loss: 0.0570128328557250, loss_fid: 0.0570128328557250\n",
      "epoch: 0270, loss: 0.0567754159747659, loss_fid: 0.0567754159747659\n",
      "epoch: 0271, loss: 0.0565418074369274, loss_fid: 0.0565418074369274\n",
      "epoch: 0272, loss: 0.0563116642846065, loss_fid: 0.0563116642846065\n",
      "epoch: 0273, loss: 0.0560829293775398, loss_fid: 0.0560829293775398\n",
      "epoch: 0274, loss: 0.0558536115104700, loss_fid: 0.0558536115104700\n",
      "epoch: 0275, loss: 0.0556235450990689, loss_fid: 0.0556235450990689\n",
      "epoch: 0276, loss: 0.0553940305220315, loss_fid: 0.0553940305220315\n",
      "epoch: 0277, loss: 0.0551666270597231, loss_fid: 0.0551666270597231\n",
      "epoch: 0278, loss: 0.0549418661915050, loss_fid: 0.0549418661915050\n",
      "epoch: 0279, loss: 0.0547193042409333, loss_fid: 0.0547193042409333\n",
      "epoch: 0280, loss: 0.0544982421230692, loss_fid: 0.0544982421230692\n",
      "epoch: 0281, loss: 0.0542786634047432, loss_fid: 0.0542786634047432\n",
      "epoch: 0282, loss: 0.0540614950662071, loss_fid: 0.0540614950662071\n",
      "epoch: 0283, loss: 0.0538484847940482, loss_fid: 0.0538484847940482\n",
      "epoch: 0284, loss: 0.0536419532064074, loss_fid: 0.0536419532064074\n",
      "epoch: 0285, loss: 0.0534451244558928, loss_fid: 0.0534451244558928\n",
      "epoch: 0286, loss: 0.0532633050238167, loss_fid: 0.0532633050238167\n",
      "epoch: 0287, loss: 0.0531055841497544, loss_fid: 0.0531055841497544\n",
      "epoch: 0288, loss: 0.0529886431108829, loss_fid: 0.0529886431108829\n",
      "epoch: 0289, loss: 0.0529390228672599, loss_fid: 0.0529390228672599\n",
      "epoch: 0290, loss: 0.0530006439104871, loss_fid: 0.0530006439104871\n",
      "epoch: 0291, loss: 0.0532219694392549, loss_fid: 0.0532219694392549\n",
      "epoch: 0292, loss: 0.0536433696962797, loss_fid: 0.0536433696962797\n",
      "epoch: 0293, loss: 0.0541690658134965, loss_fid: 0.0541690658134965\n",
      "epoch: 0294, loss: 0.0545193234412973, loss_fid: 0.0545193234412973\n",
      "epoch: 0295, loss: 0.0541567526024821, loss_fid: 0.0541567526024821\n",
      "epoch: 0296, loss: 0.0529672264651085, loss_fid: 0.0529672264651085\n",
      "epoch: 0297, loss: 0.0515079924059405, loss_fid: 0.0515079924059405\n",
      "epoch: 0298, loss: 0.0507097843000550, loss_fid: 0.0507097843000550\n",
      "epoch: 0299, loss: 0.0508425735309433, loss_fid: 0.0508425735309433\n",
      "epoch: 0300, loss: 0.0513795060667267, loss_fid: 0.0513795060667267\n",
      "epoch: 0301, loss: 0.0515585471326453, loss_fid: 0.0515585471326453\n",
      "epoch: 0302, loss: 0.0510083489147442, loss_fid: 0.0510083489147442\n",
      "epoch: 0303, loss: 0.0501238261892953, loss_fid: 0.0501238261892953\n",
      "epoch: 0304, loss: 0.0495624158523178, loss_fid: 0.0495624158523178\n",
      "epoch: 0305, loss: 0.0495693148617405, loss_fid: 0.0495693148617405\n",
      "epoch: 0306, loss: 0.0498026712550890, loss_fid: 0.0498026712550890\n",
      "epoch: 0307, loss: 0.0497695819829890, loss_fid: 0.0497695819829890\n",
      "epoch: 0308, loss: 0.0493307854309560, loss_fid: 0.0493307854309560\n",
      "epoch: 0309, loss: 0.0487667844189359, loss_fid: 0.0487667844189359\n",
      "epoch: 0310, loss: 0.0484453360087485, loss_fid: 0.0484453360087485\n",
      "epoch: 0311, loss: 0.0484259309739972, loss_fid: 0.0484259309739972\n",
      "epoch: 0312, loss: 0.0484694987664429, loss_fid: 0.0484694987664429\n",
      "epoch: 0313, loss: 0.0483291022537956, loss_fid: 0.0483291022537956\n",
      "epoch: 0314, loss: 0.0479761772389687, loss_fid: 0.0479761772389687\n",
      "epoch: 0315, loss: 0.0475969059451811, loss_fid: 0.0475969059451811\n",
      "epoch: 0316, loss: 0.0473673636648395, loss_fid: 0.0473673636648395\n",
      "epoch: 0317, loss: 0.0472935150986770, loss_fid: 0.0472935150986770\n",
      "epoch: 0318, loss: 0.0472410546505473, loss_fid: 0.0472410546505473\n",
      "epoch: 0319, loss: 0.0470842257007543, loss_fid: 0.0470842257007543\n",
      "epoch: 0320, loss: 0.0468170965940741, loss_fid: 0.0468170965940741\n",
      "epoch: 0321, loss: 0.0465305596157692, loss_fid: 0.0465305596157692\n",
      "epoch: 0322, loss: 0.0463171999630617, loss_fid: 0.0463171999630617\n",
      "epoch: 0323, loss: 0.0461906426768783, loss_fid: 0.0461906426768783\n",
      "epoch: 0324, loss: 0.0460920808713285, loss_fid: 0.0460920808713285\n",
      "epoch: 0325, loss: 0.0459540059364592, loss_fid: 0.0459540059364592\n",
      "epoch: 0326, loss: 0.0457540651211165, loss_fid: 0.0457540651211165\n",
      "epoch: 0327, loss: 0.0455238412772677, loss_fid: 0.0455238412772677\n",
      "epoch: 0328, loss: 0.0453112437939888, loss_fid: 0.0453112437939888\n",
      "epoch: 0329, loss: 0.0451423490793894, loss_fid: 0.0451423490793894\n",
      "epoch: 0330, loss: 0.0450075200165624, loss_fid: 0.0450075200165624\n",
      "epoch: 0331, loss: 0.0448767900518354, loss_fid: 0.0448767900518354\n",
      "epoch: 0332, loss: 0.0447251577314833, loss_fid: 0.0447251577314833\n",
      "epoch: 0333, loss: 0.0445470824607057, loss_fid: 0.0445470824607057\n",
      "epoch: 0334, loss: 0.0443558203143417, loss_fid: 0.0443558203143417\n",
      "epoch: 0335, loss: 0.0441696427712717, loss_fid: 0.0441696427712717\n",
      "epoch: 0336, loss: 0.0440001098012317, loss_fid: 0.0440001098012317\n",
      "epoch: 0337, loss: 0.0438472224082389, loss_fid: 0.0438472224082389\n",
      "epoch: 0338, loss: 0.0437027074393177, loss_fid: 0.0437027074393177\n",
      "epoch: 0339, loss: 0.0435568122669627, loss_fid: 0.0435568122669627\n",
      "epoch: 0340, loss: 0.0434034461071496, loss_fid: 0.0434034461071496\n",
      "epoch: 0341, loss: 0.0432421863303000, loss_fid: 0.0432421863303000\n",
      "epoch: 0342, loss: 0.0430763317438356, loss_fid: 0.0430763317438356\n",
      "epoch: 0343, loss: 0.0429104399908917, loss_fid: 0.0429104399908917\n",
      "epoch: 0344, loss: 0.0427479141326336, loss_fid: 0.0427479141326336\n",
      "epoch: 0345, loss: 0.0425901665671179, loss_fid: 0.0425901665671179\n",
      "epoch: 0346, loss: 0.0424368900383686, loss_fid: 0.0424368900383686\n",
      "epoch: 0347, loss: 0.0422867676782446, loss_fid: 0.0422867676782446\n",
      "epoch: 0348, loss: 0.0421384308085296, loss_fid: 0.0421384308085296\n",
      "epoch: 0349, loss: 0.0419907856985892, loss_fid: 0.0419907856985892\n",
      "epoch: 0350, loss: 0.0418432235564632, loss_fid: 0.0418432235564632\n",
      "epoch: 0351, loss: 0.0416954301738411, loss_fid: 0.0416954301738411\n",
      "epoch: 0352, loss: 0.0415473664630071, loss_fid: 0.0415473664630071\n",
      "epoch: 0353, loss: 0.0413991108961695, loss_fid: 0.0413991108961695\n",
      "epoch: 0354, loss: 0.0412508096448181, loss_fid: 0.0412508096448181\n",
      "epoch: 0355, loss: 0.0411026588858353, loss_fid: 0.0411026588858353\n",
      "epoch: 0356, loss: 0.0409548692544871, loss_fid: 0.0409548692544871\n",
      "epoch: 0357, loss: 0.0408077056173656, loss_fid: 0.0408077056173656\n",
      "epoch: 0358, loss: 0.0406613902369638, loss_fid: 0.0406613902369638\n",
      "epoch: 0359, loss: 0.0405161573698131, loss_fid: 0.0405161573698131\n",
      "epoch: 0360, loss: 0.0403722050516950, loss_fid: 0.0403722050516950\n",
      "epoch: 0361, loss: 0.0402297730819272, loss_fid: 0.0402297730819272\n",
      "epoch: 0362, loss: 0.0400891544884544, loss_fid: 0.0400891544884544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0363, loss: 0.0399507879200433, loss_fid: 0.0399507879200433\n",
      "epoch: 0364, loss: 0.0398154176320098, loss_fid: 0.0398154176320098\n",
      "epoch: 0365, loss: 0.0396842493690643, loss_fid: 0.0396842493690643\n",
      "epoch: 0366, loss: 0.0395594328050493, loss_fid: 0.0395594328050493\n",
      "epoch: 0367, loss: 0.0394444429851727, loss_fid: 0.0394444429851727\n",
      "epoch: 0368, loss: 0.0393455224224090, loss_fid: 0.0393455224224090\n",
      "epoch: 0369, loss: 0.0392726728494480, loss_fid: 0.0392726728494480\n",
      "epoch: 0370, loss: 0.0392437297783594, loss_fid: 0.0392437297783594\n",
      "epoch: 0371, loss: 0.0392852230585572, loss_fid: 0.0392852230585572\n",
      "epoch: 0372, loss: 0.0394407272724979, loss_fid: 0.0394407272724979\n",
      "epoch: 0373, loss: 0.0397552362930473, loss_fid: 0.0397552362930473\n",
      "epoch: 0374, loss: 0.0402697141456244, loss_fid: 0.0402697141456244\n",
      "epoch: 0375, loss: 0.0409001829134344, loss_fid: 0.0409001829134344\n",
      "epoch: 0376, loss: 0.0414157830676199, loss_fid: 0.0414157830676199\n",
      "epoch: 0377, loss: 0.0413248837915655, loss_fid: 0.0413248837915655\n",
      "epoch: 0378, loss: 0.0404488652845110, loss_fid: 0.0404488652845110\n",
      "epoch: 0379, loss: 0.0391147986564405, loss_fid: 0.0391147986564405\n",
      "epoch: 0380, loss: 0.0381557760447752, loss_fid: 0.0381557760447752\n",
      "epoch: 0381, loss: 0.0379947691075797, loss_fid: 0.0379947691075797\n",
      "epoch: 0382, loss: 0.0383591088872247, loss_fid: 0.0383591088872247\n",
      "epoch: 0383, loss: 0.0385959098382642, loss_fid: 0.0385959098382642\n",
      "epoch: 0384, loss: 0.0383022021052691, loss_fid: 0.0383022021052691\n",
      "epoch: 0385, loss: 0.0377091914000391, loss_fid: 0.0377091914000391\n",
      "epoch: 0386, loss: 0.0373216526354760, loss_fid: 0.0373216526354760\n",
      "epoch: 0387, loss: 0.0373080835538181, loss_fid: 0.0373080835538181\n",
      "epoch: 0388, loss: 0.0373560380340983, loss_fid: 0.0373560380340983\n",
      "epoch: 0389, loss: 0.0371304290850922, loss_fid: 0.0371304290850922\n",
      "epoch: 0390, loss: 0.0367118836589542, loss_fid: 0.0367118836589542\n",
      "epoch: 0391, loss: 0.0364591170773153, loss_fid: 0.0364591170773153\n",
      "epoch: 0392, loss: 0.0365005015399927, loss_fid: 0.0365005015399927\n",
      "epoch: 0393, loss: 0.0365811552036409, loss_fid: 0.0365811552036409\n",
      "epoch: 0394, loss: 0.0364065986027896, loss_fid: 0.0364065986027896\n",
      "epoch: 0395, loss: 0.0360146909495604, loss_fid: 0.0360146909495604\n",
      "epoch: 0396, loss: 0.0357200001867710, loss_fid: 0.0357200001867710\n",
      "epoch: 0397, loss: 0.0356902429920061, loss_fid: 0.0356902429920061\n",
      "epoch: 0398, loss: 0.0357658273876582, loss_fid: 0.0357658273876582\n",
      "epoch: 0399, loss: 0.0356961712946320, loss_fid: 0.0356961712946320\n",
      "epoch: 0400, loss: 0.0354396922372210, loss_fid: 0.0354396922372210\n",
      "epoch: 0401, loss: 0.0351792475191656, loss_fid: 0.0351792475191656\n",
      "epoch: 0402, loss: 0.0350631547550907, loss_fid: 0.0350631547550907\n",
      "epoch: 0403, loss: 0.0350412270641184, loss_fid: 0.0350412270641184\n",
      "epoch: 0404, loss: 0.0349696073909100, loss_fid: 0.0349696073909100\n",
      "epoch: 0405, loss: 0.0347990693436675, loss_fid: 0.0347990693436675\n",
      "epoch: 0406, loss: 0.0346134079322110, loss_fid: 0.0346134079322110\n",
      "epoch: 0407, loss: 0.0344996019149888, loss_fid: 0.0344996019149888\n",
      "epoch: 0408, loss: 0.0344432051287210, loss_fid: 0.0344432051287210\n",
      "epoch: 0409, loss: 0.0343636628863385, loss_fid: 0.0343636628863385\n",
      "epoch: 0410, loss: 0.0342197476066792, loss_fid: 0.0342197476066792\n",
      "epoch: 0411, loss: 0.0340509633373978, loss_fid: 0.0340509633373978\n",
      "epoch: 0412, loss: 0.0339189681956427, loss_fid: 0.0339189681956427\n",
      "epoch: 0413, loss: 0.0338367253791787, loss_fid: 0.0338367253791787\n",
      "epoch: 0414, loss: 0.0337644310199983, loss_fid: 0.0337644310199983\n",
      "epoch: 0415, loss: 0.0336619863498485, loss_fid: 0.0336619863498485\n",
      "epoch: 0416, loss: 0.0335303497417957, loss_fid: 0.0335303497417957\n",
      "epoch: 0417, loss: 0.0334004438402202, loss_fid: 0.0334004438402202\n",
      "epoch: 0418, loss: 0.0332945264480909, loss_fid: 0.0332945264480909\n",
      "epoch: 0419, loss: 0.0332052139649227, loss_fid: 0.0332052139649227\n",
      "epoch: 0420, loss: 0.0331096054518796, loss_fid: 0.0331096054518796\n",
      "epoch: 0421, loss: 0.0329962011546457, loss_fid: 0.0329962011546457\n",
      "epoch: 0422, loss: 0.0328741150186705, loss_fid: 0.0328741150186705\n",
      "epoch: 0423, loss: 0.0327607424557462, loss_fid: 0.0327607424557462\n",
      "epoch: 0424, loss: 0.0326636262204115, loss_fid: 0.0326636262204115\n",
      "epoch: 0425, loss: 0.0325761257084991, loss_fid: 0.0325761257084991\n",
      "epoch: 0426, loss: 0.0324874451794054, loss_fid: 0.0324874451794054\n",
      "epoch: 0427, loss: 0.0323948304402265, loss_fid: 0.0323948304402265\n",
      "epoch: 0428, loss: 0.0323076852700853, loss_fid: 0.0323076852700853\n",
      "epoch: 0429, loss: 0.0322428021785086, loss_fid: 0.0322428021785086\n",
      "epoch: 0430, loss: 0.0322199709619276, loss_fid: 0.0322199709619276\n",
      "epoch: 0431, loss: 0.0322650391796597, loss_fid: 0.0322650391796597\n",
      "epoch: 0432, loss: 0.0324196357248820, loss_fid: 0.0324196357248820\n",
      "epoch: 0433, loss: 0.0327466662497058, loss_fid: 0.0327466662497058\n",
      "epoch: 0434, loss: 0.0332953671951920, loss_fid: 0.0332953671951920\n",
      "epoch: 0435, loss: 0.0339838065887335, loss_fid: 0.0339838065887335\n",
      "epoch: 0436, loss: 0.0344212679898602, loss_fid: 0.0344212679898602\n",
      "epoch: 0437, loss: 0.0340426184942062, loss_fid: 0.0340426184942062\n",
      "epoch: 0438, loss: 0.0328432345367847, loss_fid: 0.0328432345367847\n",
      "epoch: 0439, loss: 0.0317569095346628, loss_fid: 0.0317569095346628\n",
      "epoch: 0440, loss: 0.0316081200527940, loss_fid: 0.0316081200527940\n",
      "epoch: 0441, loss: 0.0321195790364456, loss_fid: 0.0321195790364456\n",
      "epoch: 0442, loss: 0.0323104176070136, loss_fid: 0.0323104176070136\n",
      "epoch: 0443, loss: 0.0317243428782864, loss_fid: 0.0317243428782864\n",
      "epoch: 0444, loss: 0.0309553104126162, loss_fid: 0.0309553104126162\n",
      "epoch: 0445, loss: 0.0307885845497050, loss_fid: 0.0307885845497050\n",
      "epoch: 0446, loss: 0.0310881358541004, loss_fid: 0.0310881358541004\n",
      "epoch: 0447, loss: 0.0311705466053885, loss_fid: 0.0311705466053885\n",
      "epoch: 0448, loss: 0.0308269761307046, loss_fid: 0.0308269761307046\n",
      "epoch: 0449, loss: 0.0305047181110535, loss_fid: 0.0305047181110535\n",
      "epoch: 0450, loss: 0.0305223376277620, loss_fid: 0.0305223376277620\n",
      "epoch: 0451, loss: 0.0305996567663976, loss_fid: 0.0305996567663976\n",
      "epoch: 0452, loss: 0.0304042039776321, loss_fid: 0.0304042039776321\n",
      "epoch: 0453, loss: 0.0300660207847900, loss_fid: 0.0300660207847900\n",
      "epoch: 0454, loss: 0.0299275486452772, loss_fid: 0.0299275486452772\n",
      "epoch: 0455, loss: 0.0299912283491870, loss_fid: 0.0299912283491870\n",
      "epoch: 0456, loss: 0.0299608673044400, loss_fid: 0.0299608673044400\n",
      "epoch: 0457, loss: 0.0297214232118158, loss_fid: 0.0297214232118158\n",
      "epoch: 0458, loss: 0.0294833674540333, loss_fid: 0.0294833674540333\n",
      "epoch: 0459, loss: 0.0294378492263757, loss_fid: 0.0294378492263757\n",
      "epoch: 0460, loss: 0.0294888542522895, loss_fid: 0.0294888542522895\n",
      "epoch: 0461, loss: 0.0294333303562526, loss_fid: 0.0294333303562526\n",
      "epoch: 0462, loss: 0.0292526109224447, loss_fid: 0.0292526109224447\n",
      "epoch: 0463, loss: 0.0290962359479516, loss_fid: 0.0290962359479516\n",
      "epoch: 0464, loss: 0.0290453646252296, loss_fid: 0.0290453646252296\n",
      "epoch: 0465, loss: 0.0290229790167146, loss_fid: 0.0290229790167146\n",
      "epoch: 0466, loss: 0.0289360805650835, loss_fid: 0.0289360805650835\n",
      "epoch: 0467, loss: 0.0288044260759409, loss_fid: 0.0288044260759409\n",
      "epoch: 0468, loss: 0.0287035026604312, loss_fid: 0.0287035026604312\n",
      "epoch: 0469, loss: 0.0286481694086385, loss_fid: 0.0286481694086385\n",
      "epoch: 0470, loss: 0.0285856728507412, loss_fid: 0.0285856728507412\n",
      "epoch: 0471, loss: 0.0284842968745260, loss_fid: 0.0284842968745260\n",
      "epoch: 0472, loss: 0.0283751433554172, loss_fid: 0.0283751433554172\n",
      "epoch: 0473, loss: 0.0283013813523769, loss_fid: 0.0283013813523769\n",
      "epoch: 0474, loss: 0.0282588922540959, loss_fid: 0.0282588922540959\n",
      "epoch: 0475, loss: 0.0282093751683297, loss_fid: 0.0282093751683297\n",
      "epoch: 0476, loss: 0.0281361489482984, loss_fid: 0.0281361489482984\n",
      "epoch: 0477, loss: 0.0280632579997886, loss_fid: 0.0280632579997886\n",
      "epoch: 0478, loss: 0.0280236649403814, loss_fid: 0.0280236649403814\n",
      "epoch: 0479, loss: 0.0280229362682702, loss_fid: 0.0280229362682702\n",
      "epoch: 0480, loss: 0.0280489176681868, loss_fid: 0.0280489176681868\n",
      "epoch: 0481, loss: 0.0280965010988120, loss_fid: 0.0280965010988120\n",
      "epoch: 0482, loss: 0.0281884493210378, loss_fid: 0.0281884493210378\n",
      "epoch: 0483, loss: 0.0283439550676349, loss_fid: 0.0283439550676349\n",
      "epoch: 0484, loss: 0.0285771015674979, loss_fid: 0.0285771015674979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0485, loss: 0.0288451158681821, loss_fid: 0.0288451158681821\n",
      "epoch: 0486, loss: 0.0291108964254491, loss_fid: 0.0291108964254491\n",
      "epoch: 0487, loss: 0.0292525242583592, loss_fid: 0.0292525242583592\n",
      "epoch: 0488, loss: 0.0291946293842824, loss_fid: 0.0291946293842824\n",
      "epoch: 0489, loss: 0.0288189062103459, loss_fid: 0.0288189062103459\n",
      "epoch: 0490, loss: 0.0281923911237943, loss_fid: 0.0281923911237943\n",
      "epoch: 0491, loss: 0.0274754257041329, loss_fid: 0.0274754257041329\n",
      "epoch: 0492, loss: 0.0269198282428009, loss_fid: 0.0269198282428009\n",
      "epoch: 0493, loss: 0.0266721452224842, loss_fid: 0.0266721452224842\n",
      "epoch: 0494, loss: 0.0267197551356035, loss_fid: 0.0267197551356035\n",
      "epoch: 0495, loss: 0.0269282642121864, loss_fid: 0.0269282642121864\n",
      "epoch: 0496, loss: 0.0271285547365955, loss_fid: 0.0271285547365955\n",
      "epoch: 0497, loss: 0.0271947564529960, loss_fid: 0.0271947564529960\n",
      "epoch: 0498, loss: 0.0270669969718266, loss_fid: 0.0270669969718266\n",
      "epoch: 0499, loss: 0.0267883623384267, loss_fid: 0.0267883623384267\n",
      "epoch: 0500, loss: 0.0264527156440764, loss_fid: 0.0264527156440764\n",
      "epoch: 0501, loss: 0.0261765693191545, loss_fid: 0.0261765693191545\n",
      "epoch: 0502, loss: 0.0260268805247006, loss_fid: 0.0260268805247006\n",
      "epoch: 0503, loss: 0.0260033287689276, loss_fid: 0.0260033287689276\n",
      "epoch: 0504, loss: 0.0260512771098039, loss_fid: 0.0260512771098039\n",
      "epoch: 0505, loss: 0.0260983441348356, loss_fid: 0.0260983441348356\n",
      "epoch: 0506, loss: 0.0260911047355585, loss_fid: 0.0260911047355585\n",
      "epoch: 0507, loss: 0.0260075394867886, loss_fid: 0.0260075394867886\n",
      "epoch: 0508, loss: 0.0258644224875405, loss_fid: 0.0258644224875405\n",
      "epoch: 0509, loss: 0.0256938104956492, loss_fid: 0.0256938104956492\n",
      "epoch: 0510, loss: 0.0255341501892569, loss_fid: 0.0255341501892569\n",
      "epoch: 0511, loss: 0.0254105705798179, loss_fid: 0.0254105705798179\n",
      "epoch: 0512, loss: 0.0253315435498949, loss_fid: 0.0253315435498949\n",
      "epoch: 0513, loss: 0.0252890709244583, loss_fid: 0.0252890709244583\n",
      "epoch: 0514, loss: 0.0252651876431055, loss_fid: 0.0252651876431055\n",
      "epoch: 0515, loss: 0.0252403939926182, loss_fid: 0.0252403939926182\n",
      "epoch: 0516, loss: 0.0251997095314417, loss_fid: 0.0251997095314417\n",
      "epoch: 0517, loss: 0.0251375672821044, loss_fid: 0.0251375672821044\n",
      "epoch: 0518, loss: 0.0250549441842283, loss_fid: 0.0250549441842283\n",
      "epoch: 0519, loss: 0.0249593354844151, loss_fid: 0.0249593354844151\n",
      "epoch: 0520, loss: 0.0248585365094987, loss_fid: 0.0248585365094987\n",
      "epoch: 0521, loss: 0.0247604871398747, loss_fid: 0.0247604871398747\n",
      "epoch: 0522, loss: 0.0246702764460369, loss_fid: 0.0246702764460369\n",
      "epoch: 0523, loss: 0.0245906163405186, loss_fid: 0.0245906163405186\n",
      "epoch: 0524, loss: 0.0245214952543161, loss_fid: 0.0245214952543161\n",
      "epoch: 0525, loss: 0.0244609360496905, loss_fid: 0.0244609360496905\n",
      "epoch: 0526, loss: 0.0244059261848091, loss_fid: 0.0244059261848091\n",
      "epoch: 0527, loss: 0.0243534170643553, loss_fid: 0.0243534170643553\n",
      "epoch: 0528, loss: 0.0243011664985545, loss_fid: 0.0243011664985545\n",
      "epoch: 0529, loss: 0.0242476929265920, loss_fid: 0.0242476929265920\n",
      "epoch: 0530, loss: 0.0241925808810511, loss_fid: 0.0241925808810511\n",
      "epoch: 0531, loss: 0.0241355352174046, loss_fid: 0.0241355352174046\n",
      "epoch: 0532, loss: 0.0240770350506656, loss_fid: 0.0240770350506656\n",
      "epoch: 0533, loss: 0.0240172211485973, loss_fid: 0.0240172211485973\n",
      "epoch: 0534, loss: 0.0239570142110472, loss_fid: 0.0239570142110472\n",
      "epoch: 0535, loss: 0.0238968124421481, loss_fid: 0.0238968124421481\n",
      "epoch: 0536, loss: 0.0238377277686146, loss_fid: 0.0238377277686146\n",
      "epoch: 0537, loss: 0.0237801639779580, loss_fid: 0.0237801639779580\n",
      "epoch: 0538, loss: 0.0237252885094125, loss_fid: 0.0237252885094125\n",
      "epoch: 0539, loss: 0.0236735783564305, loss_fid: 0.0236735783564305\n",
      "epoch: 0540, loss: 0.0236266422589761, loss_fid: 0.0236266422589761\n",
      "epoch: 0541, loss: 0.0235854923294500, loss_fid: 0.0235854923294500\n",
      "epoch: 0542, loss: 0.0235528779503181, loss_fid: 0.0235528779503181\n",
      "epoch: 0543, loss: 0.0235308906391017, loss_fid: 0.0235308906391017\n",
      "epoch: 0544, loss: 0.0235243769380610, loss_fid: 0.0235243769380610\n",
      "epoch: 0545, loss: 0.0235370814109243, loss_fid: 0.0235370814109243\n",
      "epoch: 0546, loss: 0.0235771153271593, loss_fid: 0.0235771153271593\n",
      "epoch: 0547, loss: 0.0236493048220728, loss_fid: 0.0236493048220728\n",
      "epoch: 0548, loss: 0.0237641787041527, loss_fid: 0.0237641787041527\n",
      "epoch: 0549, loss: 0.0239204534623753, loss_fid: 0.0239204534623753\n",
      "epoch: 0550, loss: 0.0241203787969702, loss_fid: 0.0241203787969702\n",
      "epoch: 0551, loss: 0.0243319040296894, loss_fid: 0.0243319040296894\n",
      "epoch: 0552, loss: 0.0245223833113546, loss_fid: 0.0245223833113546\n",
      "epoch: 0553, loss: 0.0246029193876908, loss_fid: 0.0246029193876908\n",
      "epoch: 0554, loss: 0.0245179944712846, loss_fid: 0.0245179944712846\n",
      "epoch: 0555, loss: 0.0242041612338230, loss_fid: 0.0242041612338230\n",
      "epoch: 0556, loss: 0.0237174174332108, loss_fid: 0.0237174174332108\n",
      "epoch: 0557, loss: 0.0231595374064659, loss_fid: 0.0231595374064659\n",
      "epoch: 0558, loss: 0.0226876545583903, loss_fid: 0.0226876545583903\n",
      "epoch: 0559, loss: 0.0224038512737335, loss_fid: 0.0224038512737335\n",
      "epoch: 0560, loss: 0.0223324007026199, loss_fid: 0.0223324007026199\n",
      "epoch: 0561, loss: 0.0224212850099315, loss_fid: 0.0224212850099315\n",
      "epoch: 0562, loss: 0.0225798628676079, loss_fid: 0.0225798628676079\n",
      "epoch: 0563, loss: 0.0227155629251964, loss_fid: 0.0227155629251964\n",
      "epoch: 0564, loss: 0.0227586158849650, loss_fid: 0.0227586158849650\n",
      "epoch: 0565, loss: 0.0226811493379845, loss_fid: 0.0226811493379845\n",
      "epoch: 0566, loss: 0.0224982961000789, loss_fid: 0.0224982961000789\n",
      "epoch: 0567, loss: 0.0222626542747814, loss_fid: 0.0222626542747814\n",
      "epoch: 0568, loss: 0.0220389162309269, loss_fid: 0.0220389162309269\n",
      "epoch: 0569, loss: 0.0218777938960545, loss_fid: 0.0218777938960545\n",
      "epoch: 0570, loss: 0.0218011307106661, loss_fid: 0.0218011307106661\n",
      "epoch: 0571, loss: 0.0218008844226284, loss_fid: 0.0218008844226284\n",
      "epoch: 0572, loss: 0.0218508622641227, loss_fid: 0.0218508622641227\n",
      "epoch: 0573, loss: 0.0219201724836554, loss_fid: 0.0219201724836554\n",
      "epoch: 0574, loss: 0.0219863892444664, loss_fid: 0.0219863892444664\n",
      "epoch: 0575, loss: 0.0220364081824477, loss_fid: 0.0220364081824477\n",
      "epoch: 0576, loss: 0.0220747793389457, loss_fid: 0.0220747793389457\n",
      "epoch: 0577, loss: 0.0221111181179842, loss_fid: 0.0221111181179842\n",
      "epoch: 0578, loss: 0.0221571249021756, loss_fid: 0.0221571249021756\n",
      "epoch: 0579, loss: 0.0222163003512758, loss_fid: 0.0222163003512758\n",
      "epoch: 0580, loss: 0.0222615823804094, loss_fid: 0.0222615823804094\n",
      "epoch: 0581, loss: 0.0222610960627599, loss_fid: 0.0222610960627599\n",
      "epoch: 0582, loss: 0.0221646967913788, loss_fid: 0.0221646967913788\n",
      "epoch: 0583, loss: 0.0219724579284049, loss_fid: 0.0219724579284049\n",
      "epoch: 0584, loss: 0.0217177888017087, loss_fid: 0.0217177888017087\n",
      "epoch: 0585, loss: 0.0214672548971794, loss_fid: 0.0214672548971794\n",
      "epoch: 0586, loss: 0.0212697078210040, loss_fid: 0.0212697078210040\n",
      "epoch: 0587, loss: 0.0211386580640308, loss_fid: 0.0211386580640308\n",
      "epoch: 0588, loss: 0.0210608953556857, loss_fid: 0.0210608953556857\n",
      "epoch: 0589, loss: 0.0210155516495675, loss_fid: 0.0210155516495675\n",
      "epoch: 0590, loss: 0.0209832022527534, loss_fid: 0.0209832022527534\n",
      "epoch: 0591, loss: 0.0209503996075507, loss_fid: 0.0209503996075507\n",
      "epoch: 0592, loss: 0.0209092576650098, loss_fid: 0.0209092576650098\n",
      "epoch: 0593, loss: 0.0208511347033510, loss_fid: 0.0208511347033510\n",
      "epoch: 0594, loss: 0.0207748417154970, loss_fid: 0.0207748417154970\n",
      "epoch: 0595, loss: 0.0206793268708222, loss_fid: 0.0206793268708222\n",
      "epoch: 0596, loss: 0.0205746497609930, loss_fid: 0.0205746497609930\n",
      "epoch: 0597, loss: 0.0204750371416700, loss_fid: 0.0204750371416700\n",
      "epoch: 0598, loss: 0.0203951493358723, loss_fid: 0.0203951493358723\n",
      "epoch: 0599, loss: 0.0203422504429969, loss_fid: 0.0203422504429969\n",
      "epoch: 0600, loss: 0.0203133669071862, loss_fid: 0.0203133669071862\n",
      "epoch: 0601, loss: 0.0202970807819557, loss_fid: 0.0202970807819557\n",
      "epoch: 0602, loss: 0.0202794291159109, loss_fid: 0.0202794291159109\n",
      "epoch: 0603, loss: 0.0202491135741824, loss_fid: 0.0202491135741824\n",
      "epoch: 0604, loss: 0.0201999416537874, loss_fid: 0.0201999416537874\n",
      "epoch: 0605, loss: 0.0201334229927368, loss_fid: 0.0201334229927368\n",
      "epoch: 0606, loss: 0.0200545978431153, loss_fid: 0.0200545978431153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0607, loss: 0.0199719246541129, loss_fid: 0.0199719246541129\n",
      "epoch: 0608, loss: 0.0198920554848285, loss_fid: 0.0198920554848285\n",
      "epoch: 0609, loss: 0.0198194984715060, loss_fid: 0.0198194984715060\n",
      "epoch: 0610, loss: 0.0197555651644116, loss_fid: 0.0197555651644116\n",
      "epoch: 0611, loss: 0.0196992310876763, loss_fid: 0.0196992310876763\n",
      "epoch: 0612, loss: 0.0196484738867337, loss_fid: 0.0196484738867337\n",
      "epoch: 0613, loss: 0.0196011337678302, loss_fid: 0.0196011337678302\n",
      "epoch: 0614, loss: 0.0195555850841426, loss_fid: 0.0195555850841426\n",
      "epoch: 0615, loss: 0.0195110012580224, loss_fid: 0.0195110012580224\n",
      "epoch: 0616, loss: 0.0194672734053231, loss_fid: 0.0194672734053231\n",
      "epoch: 0617, loss: 0.0194247776680163, loss_fid: 0.0194247776680163\n",
      "epoch: 0618, loss: 0.0193842168026768, loss_fid: 0.0193842168026768\n",
      "epoch: 0619, loss: 0.0193461905842544, loss_fid: 0.0193461905842544\n",
      "epoch: 0620, loss: 0.0193112843411691, loss_fid: 0.0193112843411691\n",
      "epoch: 0621, loss: 0.0192799284355774, loss_fid: 0.0192799284355774\n",
      "epoch: 0622, loss: 0.0192528267059182, loss_fid: 0.0192528267059182\n",
      "epoch: 0623, loss: 0.0192311705127588, loss_fid: 0.0192311705127588\n",
      "epoch: 0624, loss: 0.0192172759431348, loss_fid: 0.0192172759431348\n",
      "epoch: 0625, loss: 0.0192150616746679, loss_fid: 0.0192150616746679\n",
      "epoch: 0626, loss: 0.0192307679288252, loss_fid: 0.0192307679288252\n",
      "epoch: 0627, loss: 0.0192742674859994, loss_fid: 0.0192742674859994\n",
      "epoch: 0628, loss: 0.0193592584115487, loss_fid: 0.0193592584115487\n",
      "epoch: 0629, loss: 0.0195068509739171, loss_fid: 0.0195068509739171\n",
      "epoch: 0630, loss: 0.0197407815782350, loss_fid: 0.0197407815782350\n",
      "epoch: 0631, loss: 0.0200937568849531, loss_fid: 0.0200937568849531\n",
      "epoch: 0632, loss: 0.0205766662354318, loss_fid: 0.0205766662354318\n",
      "epoch: 0633, loss: 0.0211845435752492, loss_fid: 0.0211845435752492\n",
      "epoch: 0634, loss: 0.0217977125888484, loss_fid: 0.0217977125888484\n",
      "epoch: 0635, loss: 0.0222454384837463, loss_fid: 0.0222454384837463\n",
      "epoch: 0636, loss: 0.0222415138727012, loss_fid: 0.0222415138727012\n",
      "epoch: 0637, loss: 0.0217141838325778, loss_fid: 0.0217141838325778\n",
      "epoch: 0638, loss: 0.0208526072730589, loss_fid: 0.0208526072730589\n",
      "epoch: 0639, loss: 0.0201157261218577, loss_fid: 0.0201157261218577\n",
      "epoch: 0640, loss: 0.0198805686679936, loss_fid: 0.0198805686679936\n",
      "epoch: 0641, loss: 0.0201420965510702, loss_fid: 0.0201420965510702\n",
      "epoch: 0642, loss: 0.0204892000454145, loss_fid: 0.0204892000454145\n",
      "epoch: 0643, loss: 0.0204729903217605, loss_fid: 0.0204729903217605\n",
      "epoch: 0644, loss: 0.0198599850504471, loss_fid: 0.0198599850504471\n",
      "epoch: 0645, loss: 0.0189661761665882, loss_fid: 0.0189661761665882\n",
      "epoch: 0646, loss: 0.0183427915863585, loss_fid: 0.0183427915863585\n",
      "epoch: 0647, loss: 0.0183069110201544, loss_fid: 0.0183069110201544\n",
      "epoch: 0648, loss: 0.0187141816112765, loss_fid: 0.0187141816112765\n",
      "epoch: 0649, loss: 0.0191224065007332, loss_fid: 0.0191224065007332\n",
      "epoch: 0650, loss: 0.0191704724937949, loss_fid: 0.0191704724937949\n",
      "epoch: 0651, loss: 0.0187963645244145, loss_fid: 0.0187963645244145\n",
      "epoch: 0652, loss: 0.0182826364355938, loss_fid: 0.0182826364355938\n",
      "epoch: 0653, loss: 0.0179654337864672, loss_fid: 0.0179654337864672\n",
      "epoch: 0654, loss: 0.0179716732077998, loss_fid: 0.0179716732077998\n",
      "epoch: 0655, loss: 0.0181561358198721, loss_fid: 0.0181561358198721\n",
      "epoch: 0656, loss: 0.0182738547082374, loss_fid: 0.0182738547082374\n",
      "epoch: 0657, loss: 0.0181917658311157, loss_fid: 0.0181917658311157\n",
      "epoch: 0658, loss: 0.0179658547994502, loss_fid: 0.0179658547994502\n",
      "epoch: 0659, loss: 0.0177601214809262, loss_fid: 0.0177601214809262\n",
      "epoch: 0660, loss: 0.0176898252064072, loss_fid: 0.0176898252064072\n",
      "epoch: 0661, loss: 0.0177387378579809, loss_fid: 0.0177387378579809\n",
      "epoch: 0662, loss: 0.0177976015016840, loss_fid: 0.0177976015016840\n",
      "epoch: 0663, loss: 0.0177702268837442, loss_fid: 0.0177702268837442\n",
      "epoch: 0664, loss: 0.0176442287578440, loss_fid: 0.0176442287578440\n",
      "epoch: 0665, loss: 0.0174885421865573, loss_fid: 0.0174885421865573\n",
      "epoch: 0666, loss: 0.0173821167323758, loss_fid: 0.0173821167323758\n",
      "epoch: 0667, loss: 0.0173552530229476, loss_fid: 0.0173552530229476\n",
      "epoch: 0668, loss: 0.0173773202438727, loss_fid: 0.0173773202438727\n",
      "epoch: 0669, loss: 0.0173922065489300, loss_fid: 0.0173922065489300\n",
      "epoch: 0670, loss: 0.0173627534650381, loss_fid: 0.0173627534650381\n",
      "epoch: 0671, loss: 0.0172899910335848, loss_fid: 0.0172899910335848\n",
      "epoch: 0672, loss: 0.0172042745076277, loss_fid: 0.0172042745076277\n",
      "epoch: 0673, loss: 0.0171360193982159, loss_fid: 0.0171360193982159\n",
      "epoch: 0674, loss: 0.0170966988283391, loss_fid: 0.0170966988283391\n",
      "epoch: 0675, loss: 0.0170761234813813, loss_fid: 0.0170761234813813\n",
      "epoch: 0676, loss: 0.0170551089878243, loss_fid: 0.0170551089878243\n",
      "epoch: 0677, loss: 0.0170196380527042, loss_fid: 0.0170196380527042\n",
      "epoch: 0678, loss: 0.0169684593924803, loss_fid: 0.0169684593924803\n",
      "epoch: 0679, loss: 0.0169102350587873, loss_fid: 0.0169102350587873\n",
      "epoch: 0680, loss: 0.0168559930984850, loss_fid: 0.0168559930984850\n",
      "epoch: 0681, loss: 0.0168120500400917, loss_fid: 0.0168120500400917\n",
      "epoch: 0682, loss: 0.0167779948834381, loss_fid: 0.0167779948834381\n",
      "epoch: 0683, loss: 0.0167488826295269, loss_fid: 0.0167488826295269\n",
      "epoch: 0684, loss: 0.0167191236120240, loss_fid: 0.0167191236120240\n",
      "epoch: 0685, loss: 0.0166854325287996, loss_fid: 0.0166854325287996\n",
      "epoch: 0686, loss: 0.0166475713051226, loss_fid: 0.0166475713051226\n",
      "epoch: 0687, loss: 0.0166073944623780, loss_fid: 0.0166073944623780\n",
      "epoch: 0688, loss: 0.0165672271406387, loss_fid: 0.0165672271406387\n",
      "epoch: 0689, loss: 0.0165288371060729, loss_fid: 0.0165288371060729\n",
      "epoch: 0690, loss: 0.0164929078344610, loss_fid: 0.0164929078344610\n",
      "epoch: 0691, loss: 0.0164593563652565, loss_fid: 0.0164593563652565\n",
      "epoch: 0692, loss: 0.0164277695812228, loss_fid: 0.0164277695812228\n",
      "epoch: 0693, loss: 0.0163977300226570, loss_fid: 0.0163977300226570\n",
      "epoch: 0694, loss: 0.0163690670899108, loss_fid: 0.0163690670899108\n",
      "epoch: 0695, loss: 0.0163419847017084, loss_fid: 0.0163419847017084\n",
      "epoch: 0696, loss: 0.0163172282380667, loss_fid: 0.0163172282380667\n",
      "epoch: 0697, loss: 0.0162962044821522, loss_fid: 0.0162962044821522\n",
      "epoch: 0698, loss: 0.0162815076144327, loss_fid: 0.0162815076144327\n",
      "epoch: 0699, loss: 0.0162768361249587, loss_fid: 0.0162768361249587\n",
      "epoch: 0700, loss: 0.0162883410591647, loss_fid: 0.0162883410591647\n",
      "epoch: 0701, loss: 0.0163241589681249, loss_fid: 0.0163241589681249\n",
      "epoch: 0702, loss: 0.0163975138701862, loss_fid: 0.0163975138701862\n",
      "epoch: 0703, loss: 0.0165252788827552, loss_fid: 0.0165252788827552\n",
      "epoch: 0704, loss: 0.0167332932636574, loss_fid: 0.0167332932636574\n",
      "epoch: 0705, loss: 0.0170482688816126, loss_fid: 0.0170482688816126\n",
      "epoch: 0706, loss: 0.0175004051915021, loss_fid: 0.0175004051915021\n",
      "epoch: 0707, loss: 0.0180833857395787, loss_fid: 0.0180833857395787\n",
      "epoch: 0708, loss: 0.0187460015732134, loss_fid: 0.0187460015732134\n",
      "epoch: 0709, loss: 0.0192966537379563, loss_fid: 0.0192966537379563\n",
      "epoch: 0710, loss: 0.0195037889014429, loss_fid: 0.0195037889014429\n",
      "epoch: 0711, loss: 0.0190990387836808, loss_fid: 0.0190990387836808\n",
      "epoch: 0712, loss: 0.0181655104447924, loss_fid: 0.0181655104447924\n",
      "epoch: 0713, loss: 0.0170106035415870, loss_fid: 0.0170106035415870\n",
      "epoch: 0714, loss: 0.0161314008150383, loss_fid: 0.0161314008150383\n",
      "epoch: 0715, loss: 0.0157892399376481, loss_fid: 0.0157892399376481\n",
      "epoch: 0716, loss: 0.0159474435505796, loss_fid: 0.0159474435505796\n",
      "epoch: 0717, loss: 0.0163491247208413, loss_fid: 0.0163491247208413\n",
      "epoch: 0718, loss: 0.0166920750746533, loss_fid: 0.0166920750746533\n",
      "epoch: 0719, loss: 0.0167755369428529, loss_fid: 0.0167755369428529\n",
      "epoch: 0720, loss: 0.0165506742682416, loss_fid: 0.0165506742682416\n",
      "epoch: 0721, loss: 0.0161494139942056, loss_fid: 0.0161494139942056\n",
      "epoch: 0722, loss: 0.0157548678309531, loss_fid: 0.0157548678309531\n",
      "epoch: 0723, loss: 0.0155248268684443, loss_fid: 0.0155248268684443\n",
      "epoch: 0724, loss: 0.0155037108068253, loss_fid: 0.0155037108068253\n",
      "epoch: 0725, loss: 0.0156246512191103, loss_fid: 0.0156246512191103\n",
      "epoch: 0726, loss: 0.0157640311061327, loss_fid: 0.0157640311061327\n",
      "epoch: 0727, loss: 0.0158100346602080, loss_fid: 0.0158100346602080\n",
      "epoch: 0728, loss: 0.0157179702168958, loss_fid: 0.0157179702168958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0729, loss: 0.0155214329643867, loss_fid: 0.0155214329643867\n",
      "epoch: 0730, loss: 0.0153129724291354, loss_fid: 0.0153129724291354\n",
      "epoch: 0731, loss: 0.0151800909933775, loss_fid: 0.0151800909933775\n",
      "epoch: 0732, loss: 0.0151560530058552, loss_fid: 0.0151560530058552\n",
      "epoch: 0733, loss: 0.0152096113425984, loss_fid: 0.0152096113425984\n",
      "epoch: 0734, loss: 0.0152734451697757, loss_fid: 0.0152734451697757\n",
      "epoch: 0735, loss: 0.0152855794710319, loss_fid: 0.0152855794710319\n",
      "epoch: 0736, loss: 0.0152222637313093, loss_fid: 0.0152222637313093\n",
      "epoch: 0737, loss: 0.0151052067830025, loss_fid: 0.0151052067830025\n",
      "epoch: 0738, loss: 0.0149822560914968, loss_fid: 0.0149822560914968\n",
      "epoch: 0739, loss: 0.0148966343189588, loss_fid: 0.0148966343189588\n",
      "epoch: 0740, loss: 0.0148642728777543, loss_fid: 0.0148642728777543\n",
      "epoch: 0741, loss: 0.0148715505873553, loss_fid: 0.0148715505873553\n",
      "epoch: 0742, loss: 0.0148890100733761, loss_fid: 0.0148890100733761\n",
      "epoch: 0743, loss: 0.0148895739349273, loss_fid: 0.0148895739349273\n",
      "epoch: 0744, loss: 0.0148606074410493, loss_fid: 0.0148606074410493\n",
      "epoch: 0745, loss: 0.0148062158230973, loss_fid: 0.0148062158230973\n",
      "epoch: 0746, loss: 0.0147408490225476, loss_fid: 0.0147408490225476\n",
      "epoch: 0747, loss: 0.0146801118949662, loss_fid: 0.0146801118949662\n",
      "epoch: 0748, loss: 0.0146332128979739, loss_fid: 0.0146332128979739\n",
      "epoch: 0749, loss: 0.0146011580327590, loss_fid: 0.0146011580327590\n",
      "epoch: 0750, loss: 0.0145789391138377, loss_fid: 0.0145789391138377\n",
      "epoch: 0751, loss: 0.0145597863643491, loss_fid: 0.0145597863643491\n",
      "epoch: 0752, loss: 0.0145384662558566, loss_fid: 0.0145384662558566\n",
      "epoch: 0753, loss: 0.0145126413893993, loss_fid: 0.0145126413893993\n",
      "epoch: 0754, loss: 0.0144823129832308, loss_fid: 0.0144823129832308\n",
      "epoch: 0755, loss: 0.0144488968434967, loss_fid: 0.0144488968434967\n",
      "epoch: 0756, loss: 0.0144138190784727, loss_fid: 0.0144138190784727\n",
      "epoch: 0757, loss: 0.0143782584500529, loss_fid: 0.0143782584500529\n",
      "epoch: 0758, loss: 0.0143429186677539, loss_fid: 0.0143429186677539\n",
      "epoch: 0759, loss: 0.0143083787183855, loss_fid: 0.0143083787183855\n",
      "epoch: 0760, loss: 0.0142751690118701, loss_fid: 0.0142751690118701\n",
      "epoch: 0761, loss: 0.0142438143616844, loss_fid: 0.0142438143616844\n",
      "epoch: 0762, loss: 0.0142146193193640, loss_fid: 0.0142146193193640\n",
      "epoch: 0763, loss: 0.0141875370441404, loss_fid: 0.0141875370441404\n",
      "epoch: 0764, loss: 0.0141621388914416, loss_fid: 0.0141621388914416\n",
      "epoch: 0765, loss: 0.0141376982418063, loss_fid: 0.0141376982418063\n",
      "epoch: 0766, loss: 0.0141134291758547, loss_fid: 0.0141134291758547\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    grape = Grape(taylor_terms=20, n_step=100,lr=1e-2, n_epoch=1200)\n",
    "    # grape.ibm_single_plus(duration=20)\n",
    "    # grape.ibm_X(duration=40)\n",
    "    # grape.ibm_bell_state(duration=500)\n",
    "    grape.ibm_CNOT(duration=1200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
